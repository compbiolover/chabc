---
title: "ChABC Analysis"
author: "Andrew Willems and Tian Hong"
date: "2024-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# This sets the root directory for knitr to the project directory.
knitr::opts_knit$set(root.dir = "~/Documents/Work/Phd_program/hong_lab/Projects/chabc/")
```

# Loading packages
```{r loading packcages}
pacman::p_load(broom, cluster, ComplexHeatmap, factoextra, ggforce, ggpubr, gridExtra, randomForest, reshape2, rstatix, tidyverse, viridis)
```

# Loading outcome data
```{r loading outcome data}
latency_df <- read_csv("Data/behavior_latency.csv")
errors_df <- read_csv("Data/behavior_errors.csv")
```

# Visualizing distribution of errors and latency from the outcomes files
```{r visualize initial outcome data}
# First, summary stats by cohort of the errors and latency to append to finished dataframes later
errors_ggplot <- melt(errors_df)
colnames(errors_ggplot) <- c("Day", "Cohort", "Number")

# See distribution of latency times
latency_ggplot <- melt(latency_df)
colnames(latency_ggplot) <- c("Day", "Cohort", "Latency")
latency_ggplot <- as_tibble(latency_ggplot)

# Rounding to 2 decimal places to make it easy to see
latency_ggplot <- latency_ggplot %>%
  mutate(Latency = round(Latency, digits = 2))


errors_ggplot <- as_tibble(errors_ggplot)

df_errors_summary <- errors_ggplot %>%
  group_by(Cohort) %>%
  summarize(
    total_errors = sum(Number),
    average_errors = mean(Number),
    .groups = "drop"
  )


df_latency_summary <- latency_ggplot %>%
  group_by(Cohort) %>%
  summarize(
    average_latency = mean(Latency),
    .groups = "drop"
  )

# See distribution of errors within retrieval data
h1 <- ggplot(data = errors_ggplot, aes(x = Day, y = Cohort, fill = Number)) +
  geom_tile() +
  ggtitle("Errors") +
  theme(
    panel.background = element_blank(),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 20, face = "bold"),
    axis.text = element_text(size = 18),
    legend.text = element_text(size = 18),
    legend.title = element_text(size = 20, vjust = 1),
    legend.position = "bottom"
  ) +
  coord_fixed() +
  geom_text(aes(label = Number), color = "white") +
  scale_fill_viridis(option = "C")

ggsave(plot = h1, filename = "pup_retrieval_errors.png", path = "Outputs/Plots/exploratory_plots/", device = "png", units = "in", width = 8, height = 8, dpi = 600)


h2 <- ggplot(data = latency_ggplot, aes(x = Day, y = Cohort, fill = Latency)) +
  geom_tile() +
  ggtitle("Latency") +
  theme(
    panel.background = element_blank(),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 20, face = "bold"),
    axis.text = element_text(size = 18),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 20, vjust = 1),
    legend.position = "bottom",
    legend.key.width = unit(1, "cm")
  ) +
  coord_fixed() +
  geom_text(aes(label = Latency), color = "white") +
  scale_fill_viridis(option = "C")

ggsave(plot = h2, filename = "pup_retrieval_latency.png", path = "Outputs/Plots/exploratory_plots/", device = "png", units = "in", width = 8, height = 8, dpi = 600)

# Combo plot of outcome measures
combo_outcomes <- ggarrange(h1, h2, ncol = 2, nrow = 1, labels = "AUTO", align = "h", font.label = list(size = 20, face = "bold"))

ggsave(plot = combo_outcomes, filename = "combo_outcomes.png", path = "Outputs/Plots/exploratory_plots/", device = "png", width = 12, height = 12, units = "in", dpi = 600, bg = "white")
```


# Correlation analysis
```{r correlation analysis between outcomes across cohorts}
# Remove the identifier column (e.g., 'Day') for analysis
latency_data <- latency_df %>% select(-Day)
errors_data <- errors_df %>% select(-Day)

# Prepare a data frame to store results
results_cohort <- tibble(
  cohort = colnames(latency_data),
  tau = numeric(length(colnames(latency_data))),
  p_value = numeric(length(colnames(latency_data))),
  strength = character(length(colnames(latency_data)))
)

# Perform Kendall’s Tau analysis
for (i in 1:ncol(latency_data)) {
  test_result <- cor.test(latency_data[[i]], errors_data[[i]], method = "kendall")

  # Categorizing the correlation strength based on tau value
  strength <- case_when(
    abs(test_result$estimate) < 0.3 ~ "Weak",
    abs(test_result$estimate) >= 0.3 & abs(test_result$estimate) < 0.5 ~ "Moderate",
    abs(test_result$estimate) >= 0.5 ~ "Strong"
  )
  results_cohort$tau[i] <- test_result$estimate
  results_cohort$p_value[i] <- test_result$p.value
  results_cohort$strength[i] <- strength
}

# View the results
print(results_cohort)

# Saving
pdf("Outputs/Tables/tau_correlation_across_days_by_cohort.pdf")
grid.table(results_cohort)
dev.off()
```


```{r latency across cohorts by timepoint analysis}
# Remove the identifier column (e.g., 'Day') for analysis
latency_data_t <- t(latency_df %>% select(-Day))
errors_data_t <- t(errors_df %>% select(-Day))
colnames(latency_data_t) <- latency_df$Day
colnames(errors_data_t) <- latency_df$Day


# Prepare a data frame to store results
results_day <- tibble(
  day = colnames(latency_data_t),
  tau = numeric(6),
  p_value = numeric(6),
  strength = character(6)
)

# Perform Kendall’s Tau analysis
for (i in 1:ncol(latency_data_t)) {
  test_result <- cor.test(latency_data_t[, i], errors_data_t[, i], method = "kendall")

  # Categorizing the correlation strength based on tau value
  strength <- case_when(
    abs(test_result$estimate) < 0.3 ~ "Weak",
    abs(test_result$estimate) >= 0.3 & abs(test_result$estimate) < 0.5 ~ "Moderate",
    abs(test_result$estimate) >= 0.5 ~ "Strong"
  )
  results_day$tau[i] <- test_result$estimate
  results_day$p_value[i] <- test_result$p.value
  results_day$strength[i] <- strength
}

# View the results
print(results_day)

# Saving
pdf("Outputs/Tables/tau_correlation_across_cohorts_by_day.pdf")
grid.table(results_day)
dev.off()
```

# Visualizing the results of the correlation analysis
```{r barplots}
results_cohort <- results_cohort %>%
  mutate(cohort = str_remove(cohort, "^#")) %>%
  filter(!is.na(tau))

p1 <- ggplot(data = results_cohort, aes(x = cohort, y = tau)) +
  geom_col(width = 0.5, fill = "skyblue") +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_text(angle = 45, vjust = 0.4),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste(Tau, " Across Days by Cohort"))) +
  ylab(expression(Tau)) +
  xlab("Cohort") +
  geom_hline(yintercept = 0.5, color = "red", linewidth = 0.8) +
  geom_hline(yintercept = 0.0, color = "black", linetype = "dashed", linewidth = 0.8)


p2 <- ggplot(data = results_day, aes(x = day, y = tau)) +
  geom_col(width = 0.5, fill = "skyblue") +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_text(angle = 45, vjust = 0.4),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste(Tau, " Across Cohorts by Day"))) +
  ylab(expression(Tau)) +
  xlab("Day") +
  scale_y_continuous(expand = c(0, 0)) +
  coord_cartesian(ylim = c(0.0, 0.8)) +
  geom_hline(yintercept = 0.5, color = "red", linewidth = 0.8)


# Combo plot
combo_tau <- ggarrange(p1, p2, ncol = 2, nrow = 1, labels = "AUTO", font.label = list(size = 18, face = "bold"), align = "h")

# Violin plots to see distribution with mean
results_cohort$cohort_all <- factor("All", levels = "All")

# Calculate the mean of all the taus
mean_tau <- mean(results_cohort$tau)

p3 <- ggplot(data = results_cohort, aes(x = cohort_all, y = tau)) +
  geom_violin() +
  geom_sina(jitter_y = FALSE, size = 6) +
  geom_point(data = data.frame(x = "All", y = mean_tau), aes(x = x, y = y), color = "#FF7276", fill = "#FF7276", size = 8, shape = 5) +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste("Distribution of ", Tau, " Across Cohorts"))) +
  ylab(expression(Tau)) +
  xlab("")



results_day$day_all <- factor("All", levels = "All")

# Calculate the mean of all the taus
mean_tau <- mean(results_day$tau)

p4 <- ggplot(data = results_day, aes(x = day_all, y = tau)) +
  geom_violin() +
  geom_sina(jitter_y = FALSE, size = 6) +
  geom_point(data = data.frame(x = "All", y = mean_tau), aes(x = x, y = y), color = "#FF7276", fill = "#FF7276", size = 8, shape = 5) +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste("Distribution of ", Tau, " Across Days"))) +
  ylab(expression(Tau)) +
  xlab("")


# Combo plot
combo_tau <- ggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2, labels = "AUTO", font.label = list(size = 18, face = "bold"), align = "hv")

# Saving finished plot
ggsave(filename = "tau_combo_plot.png", plot = combo_tau, path = "Outputs/Plots/exploratory_plots/", width = 12, height = 12, units = "in", dpi = 600, bg = "white", device = "png")
```


# Now creating correlation analysis between average latency, average errors, or total errors and reduction values based on our various approaches
```{r broad correlation analysis}
# Function to extract relevant data to get ready for model format
extract_columns <- function(data, file_name = "ssp_bfd") {
  # Initialize an empty list to store the extracted columns
  extracted_columns <- list()

  # Extract the subregion used by regex for later incorporation
  if (file_name != "Data/chabc_ss_s.csv") {
    subregion_name <- sub(".*/(chabc_ssp_[^./]+)\\.csv", "\\1", file_name)
    subregion_name <- sub("chabc_", "", subregion_name)
  } else {
    subregion_name <- "ss_s"
  }


  # Loop through the columns in the dataframe
  for (i in seq(1, ncol(data), by = 2)) {
    # Extract the column names in each set of two
    column_set <- data[, i:min(i + 1, ncol(data))]
    name_to_use <- names(column_set)[1]
    colnames(column_set) <- column_set[1, ]


    # Add the extracted column set to the list
    column_set$cohort <- rep(name_to_use, times = nrow(column_set))
    extracted_columns[[name_to_use]] <- column_set
  }

  extracted_df <- bind_rows(extracted_columns)
  extracted_df$subregion <- rep(subregion_name, times = nrow(extracted_df))
  colnames(extracted_df) <- tolower(colnames(extracted_df))
  extracted_df <- extracted_df[complete.cases(extracted_df), ]
  extracted_df <- filter(extracted_df, map != "Map")

  # Return the extracted dataframe
  return(extracted_df)
}

# Function to check if all reduction values are the same (k-means will fail in this case)
# Check if all values in the dataframe are the same
check_unique_value <- function(df) {
  unique_values <- unique(unlist(df))
  num_unique <- length(unique_values)
  if (num_unique == 1) {
    return(TRUE) # All values are the same
  } else {
    return(FALSE) # Data contains more than one unique value
  }
}

pred_files <- list.files(path = "Data/", pattern = "chabc_")
all_subs <- list()
for (f in pred_files) {
  current_sub <- read_csv(paste0("Data/", f))
  current_sub <- select(current_sub, 1:26)
  current_sub <- extract_columns(current_sub, file_name = paste0("Data/", f))
  all_subs[[f]] <- current_sub
}

# Binding all of them together
all_sub_df <- bind_rows(all_subs)

# Renaming % reduction column to reduction to make it easier to put in model formula
colnames(all_sub_df)[2] <- "reduction"

# Now making a list of dataframes that align with unique subregion and cohort variables
data_to_split <- all_sub_df %>% group_by(subregion, cohort)
all_dfs <- group_split(data_to_split)

process_dataframe_map <- function(df, errors_summary = NULL, latency_summary = NULL, k = 2, print_results = TRUE, plot_silhouette = TRUE) {
  # Clustering Approach (K-means)
  k <- k
  if (!check_unique_value(matrix(df$reduction))) {
    # Mean Approach
    df <- df %>%
      mutate(mean_label = if_else(map <= mean(as.numeric(map)), "very rostral", "very caudal"))

    mean_value <- mean(as.numeric(df$map))
    less_mean <- df$map <= mean_value
    more_mean <- df$map > mean_value


    # Median Approach
    df <- df %>%
      mutate(median_label = if_else(map <= median(as.numeric(map)), "very rostral", "very caudal"))

    median_value <- median(as.numeric(df$map))
    less_median <- df$map <= median_value
    more_median <- df$map > median_value


    clustering <- kmeans(matrix(df$map), centers = k)
    cluster_centers <- clustering$centers
    less_cluster <- df$map <= min(cluster_centers)
    more_cluster <- df$map > min(cluster_centers)

    df <- df %>%
      mutate(kmeans_label = if_else(map <= min(clustering$centers), "very rostral", "very caudal"))

    # Calculate silhouette scores
    silhouette_scores <- silhouette(clustering$cluster, dist(df$map))

    # Plot silhouette scores (optional)
    if (plot_silhouette) {
      png(filename = paste0("Outputs/Plots/silhouette_plots/silhouette_plot_", nrow(cluster_centers), "_clusters_subregion_", unique(df$subregion), "_", unique(df$cohort), "_cohort_map.png"), width = 8, height = 8, units = "in", res = 600, bg = "white")
      plot(silhouette_scores, main = paste0(" K-means Clustering | Clusters:  ", nrow(cluster_centers), " | ", str_to_title(unique(df$subregion)), " | ", str_to_title(unique(df$cohort))), col = "skyblue")
      dev.off()
    }


    # Print the results (optional)
    if (print_results) {
      cat("Mean Approach:\n")
      cat("Less:", df$map[less_mean], "\n")
      cat("More:", df$map[more_mean], "\n")

      max_length <- max(length(less_mean), length(more_mean), nrow(df))

      mean_cutoff_df <- data.frame(
        mean_cuttof = rep(mean_value, length.out = max_length),
        map_ids_greater_than_mean = rep(df$map[more_mean], length.out = max_length),
        map_ids_less_than_mean = rep(df$map[less_mean], length.out = max_length),
        subregion = rep(df$subregion, length.out = max_length)
      )
      write.csv(mean_cutoff_df, file = paste("Outputs/Tables/stats_tables/joseph_files/mean_cutoff_table_", unique(df$subregion), ".csv"))

      cat("Median Approach:\n")
      cat("Less:", df$map[less_median], "\n")
      cat("More:", df$map[more_median], "\n")

      max_length <- max(length(less_median), length(more_median), nrow(df))

      median_cutoff_df <- data.frame(
        median_cuttof = rep(median_value, length.out = max_length),
        map_ids_greater_than_median = rep(df$map[more_median], length.out = max_length),
        map_ids_less_than_median = rep(df$map[less_median], length.out = max_length),
        subregion = rep(df$subregion, length.out = max_length)
      )
      write.csv(mean_cutoff_df, file = paste("Outputs/Tables/stats_tables/joseph_files/median_cutoff_table_", unique(df$subregion), ".csv"))

      cat("Clustering Approach (K-means):\n")
      cat("Less:", df$map[less_cluster], "\n")
      cat("More:", df$map[more_cluster], "\n\n")

      max_length <- max(length(less_cluster), length(more_cluster), nrow(df))

      kmeans_cutoff_df <- data.frame(
        kmeans_cuttof = rep(min(clustering$centers), length.out = max_length),
        map_ids_greater_than_kmeans = rep(df$map[more_cluster], length.out = max_length),
        map_ids_less_than_kmeans = rep(df$map[less_cluster], length.out = max_length),
        subregion = rep(df$subregion, length.out = max_length)
      )
      write.csv(mean_cutoff_df, file = paste("Outputs/Tables/stats_tables/joseph_files/kmean_cutoff_table_", unique(df$subregion), ".csv"))
    }

    # Now getting average reduction by kmeans_label
    # Calculate the average reduction for each kmeans_label group
    average_reduction_kmeans <- df %>%
      filter(!is.na(map) & !is.na(as.numeric(map))) %>%
      group_by(kmeans_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_kmeans = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_kmeans, by = "kmeans_label") %>%
      rename(kmeans = average_reduction_kmeans)

    # Now getting average reduction by mean_label
    average_reduction_mean <- df %>%
      group_by(mean_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_mean = mean(reduction, na.rm = TRUE))


    # Now getting average reduction by median_label
    average_reduction_median <- df %>%
      group_by(median_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_median = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_mean, by = "mean_label") %>%
      rename(mean = average_reduction_mean)

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_median, by = "median_label") %>%
      rename(median = average_reduction_median)

    # Joining the errors and latency summaries by Cohort
    df <- df %>%
      left_join(df_errors_summary, by = c("cohort" = "Cohort")) %>%
      left_join(df_latency_summary, by = c("cohort" = "Cohort"))
  }

  # Return the processed dataframe
  return(df)
}

# Apply the function to each dataframe in the list
processed_dataframes <- lapply(all_dfs, function(df) {
  process_dataframe_map(df, errors_summary = df_errors_summary, latency_summary = df_latency_summary, k = 2, print_results = TRUE, plot_silhouette = FALSE)
})


# Remove any dataframes that contain NA values
processed_dataframes <- keep(processed_dataframes, function(df) !any(is.na(df)))
```


# Global mean calculation
```{r global mean calculation}
# Processing all cohorts and subregions together for global mean
processed_dataframe <- process_dataframe_map(df = all_sub_df, errors_summary = df_errors_summary, latency_summary = df_latency_summary, k = 2, print_results = TRUE, plot_silhouette = FALSE)


# Function to plot and calculate correlations for all cohorts within a subregion
plot_correlation_by_subregion <- function(data, x_var, y_var, calculate_correlation = TRUE, correlation_method = "spearman", plot_color = cohort, plot_shape = kmeans_label, point_size = 2, x_label, y_label, plot_title, subregion = NULL) {
  if (calculate_correlation) {
    if (is.character(data[[deparse(substitute(x_var))]])) {
      # Convert character to numeric by scaling categories
      data[[deparse(substitute(x_var))]] <- as.numeric(factor(data[[deparse(substitute(x_var))]]))
    }
    if (is.character(data[[deparse(substitute(y_var))]])) {
      # Convert character to numeric by scaling categories
      data[[deparse(substitute(y_var))]] <- as.numeric(factor(data[[deparse(substitute(y_var))]]))
    }

    correlation_result <- cor.test(data[[deparse(substitute(x_var))]], data[[deparse(substitute(y_var))]], method = correlation_method)
  }

  # Formatting the relevant label titles from the data input
  color_label <- str_to_title(deparse(substitute(plot_color)))
  shape_label <- str_to_title(deparse(substitute(plot_shape)))
  color_label <- gsub(x = color_label, pattern = "_", replacement = " ")
  shape_label <- gsub(x = shape_label, pattern = "_", replacement = " ")
  if (deparse(substitute(x_var)) == "kmeans") {
    x_label <- "Average Reduction (K-means)"
  } else {
    x_label <- str_to_title(paste("Average Reduction (", deparse(substitute(x_var)), ")"))
  }

  # Formatting subregion text for plot title
  if (!is.null(deparse(substitute(subregion)))) {
    subregion <- unique(data$subregion)
    if (subregion != "ss_s") {
      # Split the string at "_" and capitalize both "ss"
      split_string <- strsplit(subregion, "_")[[1]]
      split_string <- gsub("ss", "SS", split_string, fixed = TRUE)

      # Join the split string with "-"
      formatted_string <- paste(split_string, collapse = "-")

      subregion <- formatted_string
    } else {
      subregion <- "SSs"
    }
  } else {
    subregion <- NULL
  }

  y_label <- str_to_title(deparse(substitute(y_var)))
  y_label <- gsub(x = y_label, pattern = "_", replacement = " ")
  plot_title <- noquote(str_to_title(paste(x_label, "by", y_label, " | \n", subregion)))

  # Calculating the additional correlation data for the faceted variable (kmeans_label)
  if (deparse(substitute(x_var)) == "kmeans") {
    data_rost <- data %>% filter(kmeans_label == "very rostral")
    data_caud <- data %>% filter(kmeans_label == "very caudal")
  } else if (deparse(substitute(x_var)) == "mean") {
    data_rost <- data %>% filter(mean_label == "very rostral")
    data_caud <- data %>% filter(mean_label == "very caudal")
  } else {
    data_rost <- data %>% filter(median_label == "very rostral")
    data_caud <- data %>% filter(median_label == "very caudal")
  }


  correlation_result2 <- cor.test(data_rost[[deparse(substitute(x_var))]], data_rost[[deparse(substitute(y_var))]], method = "kendall")
  correlation_result3 <- cor.test(data_caud[[deparse(substitute(x_var))]], data_caud[[deparse(substitute(y_var))]], method = "kendall")


  if (deparse(substitute(x_var)) == "kmeans") {
    cor_res <- data.frame(method = c("tau", "tau"), p_value = c(correlation_result2$p.value, correlation_result3$p.value), estimate = c(correlation_result2$estimate, correlation_result3$estimate), kmeans_label = c("very rostral", "very caudal"), subregion = c(unique(subregion), unique(subregion)), outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))), analysis_type = c("k-means clustering", "k-means clustering"))
  } else if (deparse(substitute(x_var)) == "mean") {
    cor_res <- data.frame(method = c("tau", "tau"), p_value = c(correlation_result2$p.value, correlation_result3$p.value), estimate = c(correlation_result2$estimate, correlation_result3$estimate), mean_label = c("very rostral", "very caudal"), subregion = c(unique(subregion), unique(subregion)), outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))), analysis_type = c("mean", "mean"))
  } else {
    cor_res <- data.frame(method = c("tau", "tau"), p_value = c(correlation_result2$p.value, correlation_result3$p.value), estimate = c(correlation_result2$estimate, correlation_result3$estimate), median_label = c("very rostral", "very caudal"), subregion = c(unique(subregion), unique(subregion)), outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))), analysis_type = c("median", "median"))
  }

  cor_res <- cor_res %>%
    add_significance("p_value")


  write.csv(cor_res, file = paste0("Outputs/Tables/stats_tables/cor_results_", unique(subregion), "_", plot_title, ".csv"))


  if (deparse(substitute(x_var)) == "kmeans") {
    p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
      geom_point(aes(color = {{ plot_color }})) +
      geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
      stat_cor(method = "kendall", cor.coef.name = "tau") +
      theme(
        plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 18, face = "bold"),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 18, face = "bold"),
        strip.text = element_text(size = 18, face = "bold"),
        legend.text = element_text(size = 16),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "black"),
        panel.spacing = unit(3, "lines")
      ) +
      labs(
        x = x_label, y = y_label,
        title = plot_title,
        subtitle = ifelse(
          calculate_correlation,
          paste(
            str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
            "p-value =", signif(correlation_result$p.value, digits = 3)
          ),
          NULL
        )
      ) +
      scale_color_discrete(name = color_label) +
      scale_shape_discrete(name = shape_label) +
      guides(size = "none") +
      facet_row(~kmeans_label, space = "fixed", scales = "fixed")

    ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_not_removed_kmeans_based.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/exploratory_plots/")
  } else if (deparse(substitute(x_var)) == "mean") {
    p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
      geom_point(aes(color = {{ plot_color }})) +
      geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
      stat_cor(method = "kendall", cor.coef.name = "tau") +
      theme(
        plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 18, face = "bold"),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 18, face = "bold"),
        strip.text = element_text(size = 18, face = "bold"),
        legend.text = element_text(size = 16),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "black"),
        panel.spacing = unit(3, "lines")
      ) +
      labs(
        x = x_label, y = y_label,
        title = plot_title,
        subtitle = ifelse(
          calculate_correlation,
          paste(
            str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
            "p-value =", signif(correlation_result$p.value, digits = 3)
          ),
          NULL
        )
      ) +
      scale_color_discrete(name = color_label) +
      scale_shape_discrete(name = shape_label) +
      guides(size = "none") +
      facet_row(~mean_label, space = "fixed", scales = "fixed")

    ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_not_removed_mean_based.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/exploratory_plots/")
  } else {
    p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
      geom_point(aes(color = {{ plot_color }})) +
      geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
      stat_cor(method = "kendall", cor.coef.name = "tau") +
      theme(
        plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 18, face = "bold"),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 18, face = "bold"),
        strip.text = element_text(size = 18, face = "bold"),
        legend.text = element_text(size = 16),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "black"),
        panel.spacing = unit(3, "lines")
      ) +
      labs(
        x = x_label, y = y_label,
        title = plot_title,
        subtitle = ifelse(
          calculate_correlation,
          paste(
            str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
            "p-value =", signif(correlation_result$p.value, digits = 3)
          ),
          NULL
        )
      ) +
      scale_color_discrete(name = color_label) +
      scale_shape_discrete(name = shape_label) +
      guides(size = "none") +
      facet_row(~median_label, space = "fixed", scales = "fixed")

    ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_not_removed_median_based.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "~/Desktop")
  }
}


processed_dataframe <- processed_dataframe %>%
  filter(subregion != "ssp_m") %>%
  filter(subregion != "ssp_tr")


# For loop to step through all subregions
for (s in unique(processed_dataframe$subregion)) {
  all_processed_dfs_filtered_subregion <- processed_dataframe %>% filter(subregion == s)

  plot_correlation_by_subregion(
    data = all_processed_dfs_filtered_subregion,
    x_var = mean,
    y_var = average_errors,
    calculate_correlation = TRUE,
    correlation_method = "spearman",
    plot_color = cohort,
    plot_shape = NULL,
    subregion = all_processed_dfs_filtered_subregion$subregion
  )
}

# Heatmap for global mean calcuation
test_mat <- matrix(data = 0, nrow = 2, ncol = 6, dimnames = list(c("very rostral", "very caudal"), c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-n", "SSp-ll", "SSs")))

cor_outcome_mat <- matrix(data = 0, nrow = 2, ncol = 6, dimnames = list(c("very rostral", "very caudal"), c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-n", "SSp-ll", "SSs")))

cor_outcome_sig_mat <- matrix(data = 0, nrow = 2, ncol = 6, dimnames = list(c("very rostral", "very caudal"), c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-n", "SSp-ll", "SSs")))

comparisons <- list()


# Generate possible combinations for pairwise comparisons later
for (c in colnames(test_mat)) {
  for (r in rownames(test_mat)) {
    comparisons[[paste0(r, "_", c)]] <- paste0(r, "_", c)
  }
}

# Change line to access right folder
all_files <- list.files(path = "Outputs/Tables/stats_tables/global_mean/", pattern = "By Average Errors")
files_list <- list()
for (f in all_files) {
  current_file <- read.csv(file = paste0("Outputs/Tables/stats_tables/global_mean/", f), row.names = 1)
  files_list[[f]] <- current_file
}

cor_estimates <- bind_rows(files_list)
cor_estimates <- cor_estimates %>%
  select(-method) %>%
  mutate(row_name = paste(subregion, mean_label, sep = "_"))

# Now moving our finished calculation matrix into our heatmap matrix
for (s in unique(cor_estimates$subregion)) {
  current_sub <- filter(cor_estimates, subregion == s)
  for (l in current_sub$mean_label) {
    current_brain_region <- filter(current_sub, mean_label == l)
    test_mat[l, s] <- -1
    cor_outcome_mat[l, s] <- -1
    cor_outcome_sig_mat[l, s] <- "ns"
  }
}

outcome_type <- unique(cor_estimates$outcome_type)
outcome_type <- outcome_type %>%
  gsub(pattern = "_", replacement = " ", x = .) %>%
  str_to_title(.)

analysis_type <- unique(cor_estimates$analysis_type)
analysis_type <- analysis_type %>%
  str_to_title(.)




p1 <- Heatmap(
  matrix = test_mat, show_heatmap_legend = FALSE,
  heatmap_legend_param = list(
    title = expression(bold("Kendall's " * Tau)),
    title_gp = gpar(
      fontsize = 16,
      fontface = "bold"
    ),
    direction = "horizontal",
    title_position = "topcenter",
    width = unit(0.8, "snpc"),
    height = unit(0.4, "snpc"),
    labels_gp = gpar(fontsize = 14, fontface = "bold")
  ),
  cluster_rows = FALSE,
  cluster_columns = FALSE,
  column_names_centered = TRUE,
  column_names_side = "bottom",
  column_names_rot = 45,
  column_order = c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-n", "SSp-ll", "SSs"),
  row_order = c("very rostral", "very caudal"),
  row_title_gp = gpar(fontsize = 20, fontface = "bold"),
  row_names_gp = gpar(fontsize = 16, fontface = "bold"),
  row_names_side = "left",
  col = "black",
  na_col = "black",
  column_names_gp = gpar(
    fontsize = 16,
    fontface = "bold"
  ),
  column_title = paste0("Global Mean\n", outcome_type),
  column_title_gp = gpar(fontsize = 22, fontface = "bold"),
  cell_fun = function(j, i, x, y, width, height, fill) {
    # Now get the p-value for the current cell
    current_p_value <- cor_estimates$p_value[i]

    # Check if the p-value is less than 0.05 and not NA
    if (cor_outcome_mat[i, j] < 0.05 && !is.na(cor_outcome_mat[i, j])) {
      # Define the text to display based on significance
      sig_text <- cor_outcome_sig_mat[i, j]
      # Add the text to the heatmap cell
      grid.text(sig_text, x, y, gp = gpar(fontsize = 16, col = "black"))
    }
  },
  rect_gp = gpar(color = "white")
)

p1 <- p1 %>%
  draw(heatmap_legend_side = "bottom")
```


# Saving the finished heatmaps for global mean calculation
```{r saving finished heatmaps}
# PNG file format
png(file = paste0("Outputs/Plots/finished_plots/complex_heatmap_combined_cohorts_by_subregion_global_mean", outcome_type, ".png"), units = "in", width = 6, height = 6, res = 600)
p1 <- draw(p1, height = unit(1, "cm") * 6)
draw(p1, height = unit(1, "cm") * 6)
dev.off()


# SVG file format
svg(file = paste0("Outputs/Plots/finished_plots/complex_heatmap_combined_cohorts_by_subregion_global_mean", outcome_type, ".svg"), width = 6, height = 6)
p1 <- draw(p1, height = unit(1, "cm") * 6)
draw(p1, height = unit(1, "cm") * 6)
dev.off()
```


# Plot for using our various labels to see if it is correlated with average latency or average errors
```{r kmeans label by latency plot}
# Keep dataframes with > 4 columns
processed_dataframes <- keep(processed_dataframes, function(df) {
  ncol(df) > 4
})

all_processed_dfs <- bind_rows(processed_dataframes)
all_processed_dfs_filtered <- filter(all_processed_dfs, kmeans < 10)


# Function to plot and calculate correlations for all cohorts within a subregion
plot_correlation_by_subregion <- function(data, x_var, y_var, calculate_correlation = TRUE, correlation_method = "spearman", plot_color = cohort, plot_shape = kmeans_label, point_size = 2, x_label, y_label, plot_title, subregion = NULL) {
  if (calculate_correlation) {
    if (is.character(data[[deparse(substitute(x_var))]])) {
      # Convert character to numeric by scaling categories
      data[[deparse(substitute(x_var))]] <- as.numeric(factor(data[[deparse(substitute(x_var))]]))
    }
    if (is.character(data[[deparse(substitute(y_var))]])) {
      # Convert character to numeric by scaling categories
      data[[deparse(substitute(y_var))]] <- as.numeric(factor(data[[deparse(substitute(y_var))]]))
    }

    correlation_result <- cor.test(data[[deparse(substitute(x_var))]], data[[deparse(substitute(y_var))]], method = correlation_method)
  }

  # Formatting the relevant label titles from the data input
  color_label <- str_to_title(deparse(substitute(plot_color)))
  shape_label <- str_to_title(deparse(substitute(plot_shape)))
  color_label <- gsub(x = color_label, pattern = "_", replacement = " ")
  shape_label <- gsub(x = shape_label, pattern = "_", replacement = " ")
  if (deparse(substitute(x_var)) == "kmeans") {
    x_label <- "Average Reduction (K-means)"
  } else {
    x_label <- str_to_title(paste("Average Reduction (", deparse(substitute(x_var)), ")"))
  }

  # Formatting subregion text for plot title
  if (!is.null(deparse(substitute(subregion)))) {
    subregion <- unique(data$subregion)
    if (subregion != "ss_s") {
      # Split the string at "_" and capitalize both "ss"
      split_string <- strsplit(subregion, "_")[[1]]
      split_string <- gsub("ss", "SS", split_string, fixed = TRUE)

      # Join the split string with "-"
      formatted_string <- paste(split_string, collapse = "-")

      subregion <- formatted_string
    } else {
      subregion <- "SSs"
    }
  } else {
    subregion <- NULL
  }

  y_label <- str_to_title(deparse(substitute(y_var)))
  y_label <- gsub(x = y_label, pattern = "_", replacement = " ")
  plot_title <- noquote(str_to_title(paste(x_label, "by", y_label, " | \n", subregion)))

  # Calculating the additional correlation data for the faceted variable (x_label)
  if (deparse(substitute(x_var)) == "kmeans") {
    data_rost <- data %>% filter(kmeans_label == "very rostral")
    data_caud <- data %>% filter(kmeans_label == "very caudal")
  } else if (deparse(substitute(x_var)) == "mean") {
    data_rost <- data %>% filter(mean_label == "very rostral")
    data_caud <- data %>% filter(mean_label == "very caudal")
  } else {
    data_rost <- data %>% filter(median_label == "very rostral")
    data_caud <- data %>% filter(median_label == "very caudal")
  }


  correlation_result2 <- cor.test(data_rost[[deparse(substitute(x_var))]], data_rost[[deparse(substitute(y_var))]], method = "kendall")
  correlation_result3 <- cor.test(data_caud[[deparse(substitute(x_var))]], data_caud[[deparse(substitute(y_var))]], method = "kendall")


  if (deparse(substitute(x_var)) == "kmeans") {
    cor_res <- data.frame(method = c("tau", "tau"), p_value = c(correlation_result2$p.value, correlation_result3$p.value), estimate = c(correlation_result2$estimate, correlation_result3$estimate), kmeans_label = c("very rostral", "very caudal"), subregion = c(unique(subregion), unique(subregion)), outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))), analysis_type = c("k-means clustering", "k-means clustering"))
  } else if (deparse(substitute(x_var)) == "mean") {
    cor_res <- data.frame(method = c("tau", "tau"), p_value = c(correlation_result2$p.value, correlation_result3$p.value), estimate = c(correlation_result2$estimate, correlation_result3$estimate), mean_label = c("very rostral", "very caudal"), subregion = c(unique(subregion), unique(subregion)), outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))), analysis_type = c("mean", "mean"))
  } else {
    cor_res <- data.frame(method = c("tau", "tau"), p_value = c(correlation_result2$p.value, correlation_result3$p.value), estimate = c(correlation_result2$estimate, correlation_result3$estimate), median_label = c("very rostral", "very caudal"), subregion = c(unique(subregion), unique(subregion)), outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))), analysis_type = c("median", "median"))
  }

  cor_res <- cor_res %>%
    add_significance("p_value")


  write.csv(cor_res, file = paste0("Outputs/Tables/stats_tables/cor_results_", unique(subregion), "_", plot_title, ".csv"))


  if (deparse(substitute(x_var)) == "kmeans") {
    p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
      geom_point(aes(color = {{ plot_color }})) +
      geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
      stat_cor(method = "kendall", cor.coef.name = "tau") +
      theme(
        plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 18, face = "bold"),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 18, face = "bold"),
        strip.text = element_text(size = 18, face = "bold"),
        legend.text = element_text(size = 16),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "black"),
        panel.spacing = unit(3, "lines")
      ) +
      labs(
        x = x_label, y = y_label,
        title = plot_title,
        subtitle = ifelse(
          calculate_correlation,
          paste(
            str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
            "p-value =", signif(correlation_result$p.value, digits = 3)
          ),
          NULL
        )
      ) +
      scale_color_discrete(name = color_label) +
      scale_shape_discrete(name = shape_label) +
      guides(size = "none") +
      facet_row(~kmeans_label, space = "fixed", scales = "fixed")

    ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_not_removed_kmeans_based.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/exploratory_plots/")
  } else if (deparse(substitute(x_var)) == "mean") {
    p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
      geom_point(aes(color = {{ plot_color }})) +
      # geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
      stat_cor(method = "kendall", cor.coef.name = "tau") +
      theme(
        plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 18, face = "bold"),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 18, face = "bold"),
        strip.text = element_text(size = 18, face = "bold"),
        legend.text = element_text(size = 16),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "black"),
        panel.spacing = unit(3, "lines")
      ) +
      labs(
        x = x_label, y = y_label,
        title = plot_title,
        subtitle = ifelse(
          calculate_correlation,
          paste(
            str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
            "p-value =", signif(correlation_result$p.value, digits = 3)
          ),
          NULL
        )
      ) +
      scale_color_discrete(name = color_label) +
      scale_shape_discrete(name = shape_label) +
      guides(size = "none") +
      facet_row(~mean_label, space = "fixed", scales = "fixed")

    ggsave(plot = p, filename = paste("Outputs/Plots/plots_for_joseph/", tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_removed_mean_based.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/plots_for_joseph/")
  } else {
    p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
      geom_point(aes(color = {{ plot_color }})) +
      geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
      stat_cor(method = "kendall", cor.coef.name = "tau") +
      theme(
        plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 18, face = "bold"),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 18, face = "bold"),
        strip.text = element_text(size = 18, face = "bold"),
        legend.text = element_text(size = 16),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "black"),
        panel.spacing = unit(3, "lines")
      ) +
      labs(
        x = x_label, y = y_label,
        title = plot_title,
        subtitle = ifelse(
          calculate_correlation,
          paste(
            str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
            "p-value =", signif(correlation_result$p.value, digits = 3)
          ),
          NULL
        )
      ) +
      scale_color_discrete(name = color_label) +
      scale_shape_discrete(name = shape_label) +
      guides(size = "none") +
      facet_row(~median_label, space = "fixed", scales = "fixed")

    ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_not_removed_median_based.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/exploratory_plots/")
  }
}



# For loop to step through all subregions
for (s in unique(all_processed_dfs_filtered$subregion)) {
  # all_processed_dfs_filtered_subregion <- all_processed_dfs_filtered %>% filter(subregion == s)
  all_processed_dfs_filtered_subregion <- all_processed_dfs_filtered %>% filter(subregion == s & cohort != "#091121")

  plot_correlation_by_subregion(
    data = all_processed_dfs_filtered_subregion,
    x_var = mean,
    y_var = average_latency,
    calculate_correlation = TRUE,
    correlation_method = "spearman",
    plot_color = cohort,
    plot_shape = NULL,
    subregion = all_processed_dfs_filtered_subregion$subregion
  )
}
```


# Making heatmaps to visualize all cohorts correlation per subregion faceted by very rostral or very caudal brain map location (based on k-means clustering)
```{r heatmap kmeans}
test_mat <- matrix(data = 0, nrow = 2, ncol = 8, dimnames = list(c("very rostral", "very caudal"), c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-tr", "SSp-n", "SSp-m", "SSp-ll", "SSs")))

cor_outcome_mat <- matrix(data = 0, nrow = 2, ncol = 8, dimnames = list(c("very rostral", "very caudal"), c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-tr", "SSp-n", "SSp-m", "SSp-ll", "SSs")))

cor_outcome_sig_mat <- matrix(data = 0, nrow = 2, ncol = 8, dimnames = list(c("very rostral", "very caudal"), c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-tr", "SSp-n", "SSp-m", "SSp-ll", "SSs")))

comparisons <- list()


# Generate possible combinations for pairwise comparisons later
for (c in colnames(test_mat)) {
  for (r in rownames(test_mat)) {
    comparisons[[paste0(r, "_", c)]] <- paste0(r, "_", c)
  }
}

# Change line to access write folder
all_files <- list.files(path = "Outputs/Tables/stats_tables/joseph_files/091121_not_removed/", pattern = "By Average Latency")
files_list <- list()
for (f in all_files) {
  current_file <- read.csv(file = paste0("Outputs/Tables/stats_tables/joseph_files/091121_not_removed/", f), row.names = 1)
  files_list[[f]] <- current_file
}

cor_estimates <- bind_rows(files_list)
cor_estimates <- cor_estimates %>%
  select(-method) %>%
  mutate(row_name = paste(subregion, mean_label, sep = "_"))

# Now moving our finished calculation matrix into our heatmap matrix
for (s in unique(cor_estimates$subregion)) {
  current_sub <- filter(cor_estimates, subregion == s)
  for (l in current_sub$mean_label) {
    current_brain_region <- filter(current_sub, mean_label == l)
    test_mat[l, s] <- current_brain_region$estimate
    cor_outcome_mat[l, s] <- current_brain_region$p_value
    cor_outcome_sig_mat[l, s] <- current_brain_region$p_value.signif
  }
}

outcome_type <- unique(cor_estimates$outcome_type)
outcome_type <- outcome_type %>%
  gsub(pattern = "_", replacement = " ", x = .) %>%
  str_to_title(.)

analysis_type <- unique(cor_estimates$analysis_type)
analysis_type <- analysis_type %>%
  str_to_title(.)



# Define the color mapping
col_fun <- circlize::colorRamp2(c(-0.5, 0, 1), c("blue", "white", "red"))



p1 <- Heatmap(
  matrix = test_mat, show_heatmap_legend = TRUE,
  heatmap_legend_param = list(
    title = expression(bold("Kendall's " * Tau)),
    title_gp = gpar(
      fontsize = 16,
      fontface = "bold"
    ),
    direction = "horizontal",
    title_position = "topcenter",
    width = unit(0.8, "snpc"),
    height = unit(0.4, "snpc"),
    at = c(-0.5, 0.0, 0.5, 1.0),
    labels_gp = gpar(fontsize = 14, fontface = "bold")
  ),
  # col = c("blue", "white", "red"),
  col = col_fun,
  cluster_rows = FALSE,
  cluster_columns = FALSE,
  column_names_centered = TRUE,
  column_names_side = "bottom",
  column_names_rot = 45,
  column_order = c("SSp-bfd", "SSp-un", "SSp-ul", "SSp-tr", "SSp-n", "SSp-m", "SSp-ll", "SSs"),
  row_order = c("very rostral", "very caudal"),
  row_title_gp = gpar(fontsize = 20, fontface = "bold"),
  row_names_gp = gpar(fontsize = 16, fontface = "bold"),
  row_names_side = "left",
  na_col = "black", column_names_gp = gpar(
    fontsize = 16,
    fontface = "bold"
  ),
  column_title = paste0(analysis_type, "\n", outcome_type),
  column_title_gp = gpar(fontsize = 22, fontface = "bold"),
  cell_fun = function(j, i, x, y, width, height, fill) {
    # Now get the p-value for the current cell
    current_p_value <- cor_estimates$p_value[i]

    # Check if the p-value is less than 0.05 and not NA
    if (cor_outcome_mat[i, j] < 0.05 && !is.na(cor_outcome_mat[i, j])) {
      # Define the text to display based on significance
      sig_text <- cor_outcome_sig_mat[i, j]
      # Add the text to the heatmap cell
      grid.text(sig_text, x, y, gp = gpar(fontsize = 16, col = "black"))
    }
  },
  rect_gp = gpar(color = "black")
)

p1 <- p1 %>%
  draw(heatmap_legend_side = "bottom")
```


# Saving the finished heatmaps
```{r saving finished heatmaps}
# PNG file format
png(file = paste0("Outputs/Plots/finished_plots/complex_heatmap_combined_cohorts_by_subregion_", analysis_type, "_", outcome_type, "_091121_not_removed.png"), units = "in", width = 6, height = 6, res = 600)
p1 <- draw(p1, height = unit(1, "cm") * 6)
draw(p1, height = unit(1, "cm") * 6)
dev.off()


# SVG file format
svg(file = paste0("Outputs/Plots/finished_plots/complex_heatmap_combined_cohorts_by_subregion_", analysis_type, "_", outcome_type, "_091121_not_removed.svg"), width = 6, height = 6)
p1 <- draw(p1, height = unit(1, "cm") * 6)
draw(p1, height = unit(1, "cm") * 6)
dev.off()
```

# All subregion and cohorts together for a single correlation faceted by rostral or caudal location
```{r all cohorts and subregions together}
# Function to extract relevant data to get ready for model format
extract_columns <- function(data, file_name = "ssp_bfd") {
  # Initialize an empty list to store the extracted columns
  extracted_columns <- list()

  # Extract the subregion used by regex for later incorporation
  if (file_name != "Data/chabc_ss_s.csv") {
    subregion_name <- sub(".*/(chabc_ssp_[^./]+)\\.csv", "\\1", file_name)
    subregion_name <- sub("chabc_", "", subregion_name)
  } else {
    subregion_name <- "ss_s"
  }


  # Loop through the columns in the dataframe
  for (i in seq(1, ncol(data), by = 2)) {
    # Extract the column names in each set of two
    column_set <- data[, i:min(i + 1, ncol(data))]
    name_to_use <- names(column_set)[1]
    colnames(column_set) <- column_set[1, ]


    # Add the extracted column set to the list
    column_set$cohort <- rep(name_to_use, times = nrow(column_set))
    extracted_columns[[name_to_use]] <- column_set
  }

  extracted_df <- bind_rows(extracted_columns)
  extracted_df$subregion <- rep(subregion_name, times = nrow(extracted_df))
  colnames(extracted_df) <- tolower(colnames(extracted_df))
  extracted_df <- extracted_df[complete.cases(extracted_df), ]
  extracted_df <- filter(extracted_df, map != "Map")

  # Return the extracted dataframe
  return(extracted_df)
}

# Function to check if all reduction values are the same (k-means will fail in this case)
# Check if all values in the dataframe are the same
check_unique_value <- function(df) {
  unique_values <- unique(unlist(df))
  num_unique <- length(unique_values)
  if (num_unique == 1) {
    return(TRUE) # All values are the same
  } else {
    return(FALSE) # Data contains more than one unique value
  }
}

pred_files <- list.files(path = "Data/", pattern = "chabc_")
all_subs <- list()
for (f in pred_files) {
  current_sub <- read_csv(paste0("Data/", f))
  current_sub <- select(current_sub, 1:26)
  current_sub <- extract_columns(current_sub, file_name = paste0("Data/", f))
  all_subs[[f]] <- current_sub
}

# Binding all of them together
all_sub_df <- bind_rows(all_subs)
colnames(all_sub_df)[2] <- "reduction"
all_sub_df <- all_sub_df |>
  mutate(
    map = as.numeric(map),
    reduction = as.numeric(reduction)
  ) |>
  select(map, reduction)

all_sub_df <- all_sub_df |>
  group_by(cohort, subregion) |>
  filter(!all(reduction == 0)) |>
  ungroup()



# Read the data
behavior_errors <- read.csv("Data/behavior_errors.csv", row.names = 1)
behavior_latency <- read.csv("Data/behavior_latency.csv", row.names = 1)

# Calculate overall average errors
avg_errors <- behavior_errors %>%
  unlist() %>%
  mean(na.rm = TRUE) %>%
  data.frame(Cohort = .)


# Calculate overall average latency
avg_latency <- behavior_latency %>%
  unlist() %>%
  mean(na.rm = TRUE) %>%
  data.frame(Cohort = .)


# Function to make the k-means binning more dynamic instead of just binary
process_dataframe_map_all_subs_dynamic_kmeans <- function(df, errors_summary = NULL, latency_summary = NULL, k = 2, plot_cluster_metrics = TRUE) {
  set.seed(1714683568)
  # Check if there is unique reduction value
  if (!check_unique_value(matrix(df$reduction))) {
    # Mean and Median Approach
    df <- df %>%
      mutate(
        mean_label = if_else(map <= mean(as.numeric(map)), "very rostral", "very caudal"),
        median_label = if_else(map <= median(as.numeric(map)), "very rostral", "very caudal")
      )

    # K-means Clustering
    clustering <- kmeans(matrix(df$map), centers = k)
    # Assign cluster labels to each observation
    df$kmeans_label <- factor(clustering$cluster)

    # Calculate silhouette scores
    if (plot_cluster_metrics) {
      silhouette_scores <- silhouette(clustering$cluster, dist(df$map))
    }


    # Joining averages for reduction and summaries for errors and latency by Cohort
    # Calculate the average reduction for each kmeans_label group
    average_reduction_kmeans <- df %>%
      filter(!is.na(map) & !is.na(as.numeric(map))) %>%
      group_by(kmeans_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_kmeans = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_kmeans, by = "kmeans_label") %>%
      rename(kmeans = average_reduction_kmeans)

    # Now getting average reduction by mean_label
    average_reduction_mean <- df %>%
      group_by(mean_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_mean = mean(reduction, na.rm = TRUE))


    # Now getting average reduction by median_label
    average_reduction_median <- df %>%
      group_by(median_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_median = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_mean, by = "mean_label") %>%
      rename(mean = average_reduction_mean)

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_median, by = "median_label") %>%
      rename(median = average_reduction_median)

    # Joining the errors and latency summaries by Cohort
    df <- df %>%
      left_join(errors_summary, by = c("cohort" = "Cohort")) %>%
      left_join(latency_summary, by = c("cohort" = "Cohort"))

    df <- df %>%
      mutate(
        average_errors = as.numeric(average_errors),
        average_latency = as.numeric(average_latency),
        reduction = as.numeric(reduction)
      )

    # Return the processed dataframe
    return(df)
  }
}



# Generating the labels
processed_dataframe2 <- process_dataframe_map_all_subs_dynamic_kmeans(all_sub_df, errors_summary = df_errors_summary, latency_summary = df_latency_summary, k = 4, plot_cluster_metrics = FALSE)


plot_correlation_all_subregions <- function(data, x_var, y_var, calculate_correlation = TRUE, correlation_method = "spearman", plot_color = cohort, plot_shape = kmeans_label, point_size = 2, x_label, y_label, plot_title, subregion = NULL) {
  if (calculate_correlation) {
    if (is.character(data[[deparse(substitute(x_var))]])) {
      # Convert character to numeric by scaling categories
      data[[deparse(substitute(x_var))]] <- as.numeric(factor(data[[deparse(substitute(x_var))]]))
    }
    if (is.character(data[[deparse(substitute(y_var))]])) {
      # Convert character to numeric by scaling categories
      data[[deparse(substitute(y_var))]] <- as.numeric(factor(data[[deparse(substitute(y_var))]]))
    }
    correlation_result <- cor.test(data[[deparse(substitute(x_var))]], data[[deparse(substitute(y_var))]], method = correlation_method)
  }

  # Formatting the relevant label titles from the data input
  color_label <- str_to_title(deparse(substitute(plot_color)))
  shape_label <- str_to_title(deparse(substitute(plot_shape)))
  color_label <- gsub(x = color_label, pattern = "_", replacement = " ")
  shape_label <- gsub(x = shape_label, pattern = "_", replacement = " ")

  if (deparse(substitute(x_var)) == "kmeans") {
    x_label <- "Average Reduction (K-means)"
  } else {
    x_label <- str_to_title(paste("Average Reduction (", deparse(substitute(x_var)), ")"))
  }

  y_label <- str_to_title(deparse(substitute(y_var)))
  y_label <- gsub(x = y_label, pattern = "_", replacement = " ")

  plot_title <- noquote(str_to_title(paste(x_label, "by", y_label, " | \n All Subregions")))

  # Calculating the additional correlation data for the faceted variable (kmeans_label)
  if (deparse(substitute(x_var)) == "kmeans") {
    data_rost <- data %>% filter(kmeans_label == "very rostral")
    data_caud <- data %>% filter(kmeans_label == "very caudal")
  } else if (deparse(substitute(x_var)) == "mean") {
    data_rost <- data %>% filter(mean_label == "very rostral")
    data_caud <- data %>% filter(mean_label == "very caudal")
  } else {
    data_rost <- data %>% filter(median_label == "very rostral")
    data_caud <- data %>% filter(median_label == "very caudal")
  }


  correlation_result2 <- cor.test(data_rost[[deparse(substitute(x_var))]], data_rost[[deparse(substitute(y_var))]], method = "kendall")
  correlation_result3 <- cor.test(data_caud[[deparse(substitute(x_var))]], data_caud[[deparse(substitute(y_var))]], method = "kendall")

  cor_res <- data.frame(
    method = c("kendall", "kendall"),
    p_value = c(correlation_result2$p.value, correlation_result3$p.value),
    estimate = c(correlation_result2$estimate, correlation_result3$estimate),
    kmeans_label = c("very rostral", "very caudal"),
    subregion = c("All Subregions", "All Subregions"),
    outcome_type = c(unique(deparse(substitute(y_var))), unique(deparse(substitute(y_var)))),
    analysis_type = c(
      ifelse(deparse(substitute(x_var)) == "kmeans", "k-means clustering", deparse(substitute(x_var))),
      ifelse(deparse(substitute(x_var)) == "kmeans", "k-means clustering", deparse(substitute(x_var)))
    )
  )

  cor_res <- cor_res %>%
    add_significance("p_value")

  write.csv(cor_res, file = paste0("Outputs/Tables/stats_tables/cor_results_all_subregions_and_all_cohorts_", plot_title, ".csv"))

  p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, shape = {{ plot_shape }}, size = point_size)) +
    geom_point(aes(color = {{ plot_color }})) +
    geom_smooth(method = "lm", linewidth = 0.75, aes(color = NULL)) +
    stat_cor(method = "kendall", cor.coef.name = "tau") +
    theme(
      plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 18, hjust = 0.5),
      axis.title = element_text(size = 18, face = "bold"),
      axis.text = element_text(size = 16),
      legend.title = element_text(size = 18, face = "bold"),
      strip.text = element_text(size = 18, face = "bold"),
      legend.text = element_text(size = 16),
      panel.background = element_blank(),
      panel.grid.major.x = element_line(color = "black"),
      panel.grid.major.y = element_line(color = "black"),
      panel.spacing = unit(3, "lines")
    ) +
    labs(
      x = x_label, y = y_label,
      title = plot_title,
      subtitle = ifelse(
        calculate_correlation,
        paste(
          str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
          "p-value =", signif(correlation_result$p.value, digits = 3)
        ),
        NULL
      )
    ) +
    scale_color_discrete(name = color_label) +
    scale_shape_discrete(name = shape_label) +
    guides(size = "none") +
    facet_row(~kmeans_label, space = "fixed", scales = "fixed")

  ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_all_cohorts_map_based_separation_091121_not_removed_all_subregions_and_cohorts.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/finished_plots/")
}
```


# Regression over all subregions
```{r regression over all subregions}
# random_seed <- 1714683568 # 1714683568
# set.seed(random_seed)

processed_dataframe2 <- processed_dataframe2 |>
  mutate(reduction_bin = ntile(reduction, n = n_distinct(kmeans_label))) |>
  mutate(
    reduction_bin = factor(reduction_bin),
    kmeans_label = factor(kmeans_label),
    mean_label = factor(mean_label),
    median_label = factor(median_label),
    reduction_bin = factor(reduction_bin)
  )



# Build and fit the regression model
processed_dataframe2 <- processed_dataframe2 |>
  drop_na()

kmeans_mod <- lm(average_latency ~ kmeans_label, data = processed_dataframe2)
summary(kmeans_mod) # p-value: 0.002239, clusters 3 and 4 have p-value < 0.05

mean_mod <- lm(average_latency ~ mean_label, data = processed_dataframe2)
summary(mean_mod) # p-value: 0.00126

median_mod <- lm(average_latency ~ median_label, data = processed_dataframe2)
summary(median_mod) # p-value: 0.000749

# Calculating RMSE for kmeans model
RSS <- c(crossprod(kmeans_mod$residuals))
MSE <- RSS / length(kmeans_mod$residuals)
RMSE <- sqrt(MSE)
print(RMSE) # 0.1428596

# Calculating RMSE for mean model
RSS <- c(crossprod(mean_mod$residuals))
MSE <- RSS / length(mean_mod$residuals)
RMSE <- sqrt(MSE)
print(RMSE) #  0.1430648

# Calculating RMSE for median model
RSS <- c(crossprod(median_mod$residuals))
MSE <- RSS / length(median_mod$residuals)
RMSE <- sqrt(MSE)
print(RMSE) #  0.1430173



# Just using reduction value to predict
reduction_mod <- lm(average_latency ~ reduction, data = processed_dataframe2)
summary(reduction_mod) # p-value:  0.168    

# Calculating RMSE for reduction model
RSS <- c(crossprod(reduction_mod$residuals))
MSE <- RSS / length(reduction_mod$residuals)
RMSE <- sqrt(MSE)
print(RMSE) # 0.1434846


# Calculating RMSE for mean model
RSS <- c(crossprod(mean_mod$residuals))
MSE <- RSS / length(mean_mod$residuals)
RMSE <- sqrt(MSE)
print(RMSE) #  0.1430648

# Calculating RMSE for median model
RSS <- c(crossprod(median_mod$residuals))
MSE <- RSS / length(median_mod$residuals)
RMSE <- sqrt(MSE)
print(RMSE) #  0.1430173

```

# Random Forest model
```{r random forest model}
# Function to build and evaluate a random forest model
build_rf_model <- function(data, formula) {
  set.seed(1715030415)  # Set a seed for reproducibility
  
  rf_model <- randomForest(formula, data = data)
  
  # Find the number of trees that produce the lowest test MSE
  best_ntree <- which.min(rf_model$mse)
  
  # Calculate the RMSE of the best model
  rmse <- sqrt(rf_model$mse[best_ntree])
  
  return(list(model = rf_model, ntree = best_ntree, rmse = rmse))
}

# Function to process the dataframe and build random forest models
process_and_build_rf_models <- function(dataframe) {
  # Process the dataframe
  processed_dataframe <- dataframe |>
    drop_na()
  
  # Build and evaluate random forest models
  kmeans_model <- build_rf_model(processed_dataframe, average_latency ~ kmeans_label)
  median_model <- build_rf_model(processed_dataframe, average_latency ~ median_label)
  mean_model <- build_rf_model(processed_dataframe, average_latency ~ mean_label)
  
  # Return a list of models with their respective RMSEs
  return(list(
    kmeans_model = kmeans_model,
    median_model = median_model,
    mean_model = mean_model
  ))
}

# Using the function now
rf_models <- process_and_build_rf_models(processed_dataframe2)

# Access the models and their RMSEs
kmeans_model <- rf_models$kmeans_model
kmeans_rmse <- kmeans_model$rmse
print(paste("K-means RMSE:", kmeans_rmse))

median_model <- rf_models$median_model
median_rmse <- median_model$rmse
print(paste("Median RMSE:", median_rmse))

mean_model <- rf_models$mean_model
mean_rmse <- mean_model$rmse
print(paste("Mean RMSE:", mean_rmse))
```
