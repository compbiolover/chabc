---
title: "ChABC Analysis"
author: "Andrew Willems and Tian Hong"
date: "2024-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# This sets the root directory for knitr to the project directory.
knitr::opts_knit$set(root.dir = "~/Documents/Work/Phd_program/hong_lab/Projects/chabc/")
```

# Loading packages
```{r loading packcages}
pacman::p_load(broom, cluster, ggforce, ggpubr, gridExtra, reshape2, tidyverse, viridis)
```

# Loading outcome data
```{r loading outcome data}
latency_df <- read_csv("Data/behavior_latency.csv")
errors_df <- read_csv("Data/behavior_errors.csv")
```

# Visualizing distribution of errors and latency from the outcomes files
```{r visualize initial outcome data}
# First, summary stats by cohort of the errors and latency to append to finished dataframes later
errors_ggplot <- melt(errors_df)
colnames(errors_ggplot) <- c("Day", "Cohort", "Number")

# See distribution of latency times
latency_ggplot <- melt(latency_df)
colnames(latency_ggplot) <- c("Day", "Cohort", "Latency")
latency_ggplot <- as_tibble(latency_ggplot)

# Rounding to 2 decimal places to make it easy to see
latency_ggplot <- latency_ggplot %>%
  mutate(Latency = round(Latency, digits = 2))


errors_ggplot <- as_tibble(errors_ggplot)

df_errors_summary <- errors_ggplot %>%
  group_by(Cohort) %>%
  summarize(
    total_errors = sum(Number),
    avg_errors = mean(Number),
    .groups = "drop"
  )


df_latency_summary <- latency_ggplot %>%
  group_by(Cohort) %>%
  summarize(
    avg_latency = mean(Latency),
    .groups = "drop"
  )

# See distribution of errors within retrieval data
h1 <- ggplot(data = errors_ggplot, aes(x = Day, y = Cohort, fill = Number)) +
  geom_tile() +
  ggtitle("Errors") +
  theme(
    panel.background = element_blank(),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 20, face = "bold"),
    axis.text = element_text(size = 18),
    legend.text = element_text(size = 18),
    legend.title = element_text(size = 20, vjust = 1),
    legend.position = "bottom"
  ) +
  coord_fixed() +
  geom_text(aes(label = Number), color = "white") +
  scale_fill_viridis(option = "C")

ggsave(plot = h1, filename = "pup_retrieval_errors.png", path = "Outputs/Plots/exploratory_plots/", device = "png", units = "in", width = 8, height = 8, dpi = 600)


h2 <- ggplot(data = latency_ggplot, aes(x = Day, y = Cohort, fill = Latency)) +
  geom_tile() +
  ggtitle("Latency") +
  theme(
    panel.background = element_blank(),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 20, face = "bold"),
    axis.text = element_text(size = 18),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 20, vjust = 1),
    legend.position = "bottom",
    legend.key.width = unit(1, "cm")
  ) +
  coord_fixed() +
  geom_text(aes(label = Latency), color = "white") +
  scale_fill_viridis(option = "C")

ggsave(plot = h2, filename = "pup_retrieval_latency.png", path = "Outputs/Plots/exploratory_plots/", device = "png", units = "in", width = 8, height = 8, dpi = 600)

# Combo plot of outcome measures
combo_outcomes <- ggarrange(h1, h2, ncol = 2, nrow = 1, labels = "AUTO", align = "h", font.label = list(size = 20, face = "bold"))

ggsave(plot = combo_outcomes, filename = "combo_outcomes.png", path = "Outputs/Plots/exploratory_plots/", device = "png", width = 12, height = 12, units = "in", dpi = 600, bg = "white")
```


# Correlation analysis
```{r correlation analysis between outcomes across cohorts}
# Remove the identifier column (e.g., 'Day') for analysis
latency_data <- latency_df %>% select(-Day)
errors_data <- errors_df %>% select(-Day)

# Prepare a data frame to store results
results_cohort <- tibble(
  cohort = colnames(latency_data),
  tau = numeric(length(colnames(latency_data))),
  p_value = numeric(length(colnames(latency_data))),
  strength = character(length(colnames(latency_data)))
)

# Perform Kendall’s Tau analysis
for (i in 1:ncol(latency_data)) {
  test_result <- cor.test(latency_data[[i]], errors_data[[i]], method = "kendall")

  # Categorizing the correlation strength based on tau value
  strength <- case_when(
    abs(test_result$estimate) < 0.3 ~ "Weak",
    abs(test_result$estimate) >= 0.3 & abs(test_result$estimate) < 0.5 ~ "Moderate",
    abs(test_result$estimate) >= 0.5 ~ "Strong"
  )
  results_cohort$tau[i] <- test_result$estimate
  results_cohort$p_value[i] <- test_result$p.value
  results_cohort$strength[i] <- strength
}

# View the results
print(results_cohort)

# Saving
pdf("Outputs/tau_correlation_across_days_by_cohort.pdf")
grid.table(results_cohort)
dev.off()
```


```{r latency across cohorts by timepoint analysis}
# Remove the identifier column (e.g., 'Day') for analysis
latency_data_t <- t(latency_df %>% select(-Day))
errors_data_t <- t(errors_df %>% select(-Day))
colnames(latency_data_t) <- latency_df$Day
colnames(errors_data_t) <- latency_df$Day


# Prepare a data frame to store results
results_day <- tibble(
  day = colnames(latency_data_t),
  tau = numeric(6),
  p_value = numeric(6),
  strength = character(6)
)

# Perform Kendall’s Tau analysis
for (i in 1:ncol(latency_data_t)) {
  test_result <- cor.test(latency_data_t[, i], errors_data_t[, i], method = "kendall")

  # Categorizing the correlation strength based on tau value
  strength <- case_when(
    abs(test_result$estimate) < 0.3 ~ "Weak",
    abs(test_result$estimate) >= 0.3 & abs(test_result$estimate) < 0.5 ~ "Moderate",
    abs(test_result$estimate) >= 0.5 ~ "Strong"
  )
  results_day$tau[i] <- test_result$estimate
  results_day$p_value[i] <- test_result$p.value
  results_day$strength[i] <- strength
}

# View the results
print(results_day)

# Saving
pdf("Outputs/tau_correlation_across_cohorts_by_day.pdf")
grid.table(results_day)
dev.off()
```

# Visualizing the results of the correlation analysis
```{r barplots}
results_cohort <- results_cohort %>%
  mutate(cohort = str_remove(cohort, "^#")) %>%
  filter(!is.na(tau))

p1 <- ggplot(data = results_cohort, aes(x = cohort, y = tau)) +
  geom_col(width = 0.5, fill = "skyblue") +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_text(angle = 45, vjust = 0.4),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste(Tau, " Across Days by Cohort"))) +
  ylab(expression(Tau)) +
  xlab("Cohort") +
  geom_hline(yintercept = 0.5, color = "red", linewidth = 0.8) +
  geom_hline(yintercept = 0.0, color = "black", linetype = "dashed", linewidth = 0.8)


p2 <- ggplot(data = results_day, aes(x = day, y = tau)) +
  geom_col(width = 0.5, fill = "skyblue") +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_text(angle = 45, vjust = 0.4),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste(Tau, " Across Cohorts by Day"))) +
  ylab(expression(Tau)) +
  xlab("Day") +
  scale_y_continuous(expand = c(0, 0)) +
  coord_cartesian(ylim = c(0.0, 0.8)) +
  geom_hline(yintercept = 0.5, color = "red", linewidth = 0.8)


# Combo plot
combo_tau <- ggarrange(p1, p2, ncol = 2, nrow = 1, labels = "AUTO", font.label = list(size = 18, face = "bold"), align = "h")

# Violin plots to see distribution with mean
results_cohort$cohort_all <- factor("All", levels = "All")

# Calculate the mean of all the taus
mean_tau <- mean(results_cohort$tau)

p3 <- ggplot(data = results_cohort, aes(x = cohort_all, y = tau)) +
  geom_violin() +
  geom_sina(jitter_y = FALSE, size = 6) +
  geom_point(data = data.frame(x = "All", y = mean_tau), aes(x = x, y = y), color = "#FF7276", fill = "#FF7276", size = 8, shape = 5) +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste("Distribution of ", Tau, " Across Cohorts"))) +
  ylab(expression(Tau)) +
  xlab("")



results_day$day_all <- factor("All", levels = "All")

# Calculate the mean of all the taus
mean_tau <- mean(results_day$tau)

p4 <- ggplot(data = results_day, aes(x = day_all, y = tau)) +
  geom_violin() +
  geom_sina(jitter_y = FALSE, size = 6) +
  geom_point(data = data.frame(x = "All", y = mean_tau), aes(x = x, y = y), color = "#FF7276", fill = "#FF7276", size = 8, shape = 5) +
  theme(
    panel.background = element_blank(),
    axis.title = element_text(size = 20, face = "bold"),
    plot.title = element_text(size = 22, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 18),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks = element_line(linewidth = 0.8),
    axis.ticks.length = unit(0.25, "cm"),
    axis.line = element_line(color = "black", linewidth = 1.25)
  ) +
  ggtitle(expression(paste("Distribution of ", Tau, " Across Days"))) +
  ylab(expression(Tau)) +
  xlab("")


# Combo plot
combo_tau <- ggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2, labels = "AUTO", font.label = list(size = 18, face = "bold"), align = "hv")

# Saving finished plot
ggsave(filename = "tau_combo_plot.png", plot = combo_tau, path = "Outputs/Plots/exploratory_plots/", width = 12, height = 12, units = "in", dpi = 600, bg = "white", device = "png")
```


# Now creating correlation analysis between average latency, average errors, or total errors and reduction values based on our various approaches
```{r broad correlation analysis}
# Function to extract relevant data to get ready for model format
extract_columns <- function(data, file_name = "ssp_bfd") {
  # Initialize an empty list to store the extracted columns
  extracted_columns <- list()

  # Extract the subregion used by regex for later incorporation
  if (file_name != "Data/chabc_ss_s.csv") {
    subregion_name <- sub(".*/(chabc_ssp_[^./]+)\\.csv", "\\1", file_name)
    subregion_name <- sub("chabc_", "", subregion_name)
  } else {
    subregion_name <- "ss_s"
  }


  # Loop through the columns in the dataframe
  for (i in seq(1, ncol(data), by = 2)) {
    # Extract the column names in each set of two
    column_set <- data[, i:min(i + 1, ncol(data))]
    name_to_use <- names(column_set)[1]
    colnames(column_set) <- column_set[1, ]


    # Add the extracted column set to the list
    column_set$cohort <- rep(name_to_use, times = nrow(column_set))
    extracted_columns[[name_to_use]] <- column_set
  }

  extracted_df <- bind_rows(extracted_columns)
  extracted_df$subregion <- rep(subregion_name, times = nrow(extracted_df))
  colnames(extracted_df) <- tolower(colnames(extracted_df))
  extracted_df <- extracted_df[complete.cases(extracted_df), ]
  extracted_df <- filter(extracted_df, map != "Map")

  # Return the extracted dataframe
  return(extracted_df)
}

# Function to check if all reduction values are the same (k-means will fail in this case)
# Check if all values in the dataframe are the same
check_unique_value <- function(df) {
  unique_values <- unique(unlist(df))
  num_unique <- length(unique_values)
  if (num_unique == 1) {
    return(TRUE) # All values are the same
  } else {
    return(FALSE) # Data contains more than one unique value
  }
}

pred_files <- list.files(path = "Data/", pattern = "chabc_")
all_subs <- list()
for (f in pred_files) {
  current_sub <- read_csv(paste0("Data/", f))
  current_sub <- select(current_sub, 1:26)
  current_sub <- extract_columns(current_sub, file_name = paste0("Data/", f))
  all_subs[[f]] <- current_sub
}

# Binding all of them together
all_sub_df <- bind_rows(all_subs)

# Renaming % reduction column to reduction to make it easier to put in model formula
colnames(all_sub_df)[2] <- "reduction"

# Now making a list of dataframes that align with unique subregion and cohort variables
data_to_split <- all_sub_df %>% group_by(subregion, cohort)
all_dfs <- group_split(data_to_split)

# List of dataframes that are only focused on unique subregions
data_to_split2 <- all_sub_df %>% group_by(subregion)
all_dfs2 <- group_split(data_to_split2)

# Now trying to apply this to all the dataframes in the list
process_dataframe_reduction <- function(df, errors_summary = NULL, latency_summary = NULL, k = 2, print_results = TRUE, plot_silhouette = TRUE) {
  # Clustering Approach (K-means)
  k <- k
  if (!check_unique_value(matrix(df$reduction))) {
    # Mean Approach
    df <- df %>%
      mutate(mean_label = if_else(reduction <= mean(as.numeric(reduction)), "less", "more"))

    mean_value <- mean(as.numeric(df$reduction))
    less_mean <- df$reduction <= mean_value
    more_mean <- df$reduction > mean_value


    # Median Approach
    df <- df %>%
      mutate(median_label = if_else(reduction <= median(as.numeric(reduction)), "less", "more"))

    median_value <- median(as.numeric(df$reduction))
    less_median <- df$reduction <= median_value
    more_median <- df$reduction > median_value

    clustering <- kmeans(matrix(df$reduction), centers = k)
    cluster_centers <- clustering$centers
    less_cluster <- df$reduction <= min(cluster_centers)
    more_cluster <- df$reduction > min(cluster_centers)

    df <- df %>%
      mutate(kmeans_label = if_else(reduction <= min(clustering$centers), "less", "more"))

    # Calculate silhouette scores
    silhouette_scores <- silhouette(clustering$cluster, dist(df$reduction))

    # Plot silhouette scores (optional)
    if (plot_silhouette) {
      png(filename = paste0("Outputs/Plots/silhouette_plots/silhouette_plot_", nrow(cluster_centers), "_clusters_subregion_", unique(df$subregion), "_", unique(df$cohort), "_cohort_reduction.png"), width = 8, height = 8, units = "in", res = 600, bg = "white")
      plot(silhouette_scores, main = paste0(" K-means Clustering | Clusters:  ", nrow(cluster_centers), " | ", str_to_title(unique(df$subregion)), " | ", str_to_title(unique(df$cohort))), col = "skyblue")
      dev.off()
    }


    # Print the results (optional)
    if (print_results) {
      cat("Mean Approach:\n")
      cat("Less:", df$reduction[less_mean], "\n")
      cat("More:", df$reduction[more_mean], "\n")

      cat("Median Approach:\n")
      cat("Less:", df$reduction[less_median], "\n")
      cat("More:", df$reduction[more_median], "\n")

      cat("Clustering Approach (K-means):\n")
      cat("Less:", df$reduction[less_cluster], "\n")
      cat("More:", df$reduction[more_cluster], "\n\n")
    }

    # Now getting average reduction by kmeans_label
    # Calculate the average reduction for each kmeans_label group
    average_reduction_kmeans <- df %>%
      filter(!is.na(reduction) & !is.na(as.numeric(reduction))) %>%
      group_by(kmeans_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_kmeans = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_kmeans, by = "kmeans_label") %>%
      rename(average_reduction_by_kmeans = average_reduction_kmeans)

    # Now getting average reduction by mean_label
    average_reduction_mean <- df %>%
      group_by(mean_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_mean = mean(reduction, na.rm = TRUE))


    # Now getting average reduction by median_label
    average_reduction_median <- df %>%
      group_by(median_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_median = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_mean, by = "mean_label") %>%
      rename(average_reduction_by_mean = average_reduction_mean)

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_median, by = "median_label") %>%
      rename(average_reduction_by_median = average_reduction_median)

    # Joining the errors and latency summaries by Cohort
    df <- df %>%
      left_join(df_errors_summary, by = c("cohort" = "Cohort")) %>%
      left_join(df_latency_summary, by = c("cohort" = "Cohort"))
  }

  # Return the processed dataframe
  return(df)
}

process_dataframe_map <- function(df, errors_summary = NULL, latency_summary = NULL, k = 2, print_results = TRUE, plot_silhouette = TRUE) {
  # Clustering Approach (K-means)
  k <- k
  if (!check_unique_value(matrix(df$reduction))) {
    # Mean Approach
    df <- df %>%
      mutate(mean_label = if_else(map <= mean(as.numeric(map)), "very rostral", "very caudal"))

    mean_value <- mean(as.numeric(df$map))
    less_mean <- df$map <= mean_value
    more_mean <- df$map > mean_value


    # Median Approach
    df <- df %>%
      mutate(median_label = if_else(map <= median(as.numeric(map)), "very rostral", "very caudal"))

    median_value <- median(as.numeric(df$map))
    less_median <- df$map <= median_value
    more_median <- df$map > median_value

    clustering <- kmeans(matrix(df$map), centers = k)
    cluster_centers <- clustering$centers
    less_cluster <- df$map <= min(cluster_centers)
    more_cluster <- df$map > min(cluster_centers)

    df <- df %>%
      mutate(kmeans_label = if_else(map <= min(clustering$centers), "very rostral", "very caudal"))

    # Calculate silhouette scores
    silhouette_scores <- silhouette(clustering$cluster, dist(df$map))

    # Plot silhouette scores (optional)
    if (plot_silhouette) {
      png(filename = paste0("Outputs/Plots/silhouette_plots/silhouette_plot_", nrow(cluster_centers), "_clusters_subregion_", unique(df$subregion), "_", unique(df$cohort), "_cohort_map.png"), width = 8, height = 8, units = "in", res = 600, bg = "white")
      plot(silhouette_scores, main = paste0(" K-means Clustering | Clusters:  ", nrow(cluster_centers), " | ", str_to_title(unique(df$subregion)), " | ", str_to_title(unique(df$cohort))), col = "skyblue")
      dev.off()
    }


    # Print the results (optional)
    if (print_results) {
      cat("Mean Approach:\n")
      cat("Less:", df$map[less_mean], "\n")
      cat("More:", df$map[more_mean], "\n")

      cat("Median Approach:\n")
      cat("Less:", df$map[less_median], "\n")
      cat("More:", df$map[more_median], "\n")

      cat("Clustering Approach (K-means):\n")
      cat("Less:", df$map[less_cluster], "\n")
      cat("More:", df$map[more_cluster], "\n\n")
    }

    # Now getting average reduction by kmeans_label
    # Calculate the average reduction for each kmeans_label group
    average_reduction_kmeans <- df %>%
      filter(!is.na(map) & !is.na(as.numeric(map))) %>%
      group_by(kmeans_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_kmeans = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_kmeans, by = "kmeans_label") %>%
      rename(average_reduction_by_kmeans = average_reduction_kmeans)

    # Now getting average reduction by mean_label
    average_reduction_mean <- df %>%
      group_by(mean_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_mean = mean(reduction, na.rm = TRUE))


    # Now getting average reduction by median_label
    average_reduction_median <- df %>%
      group_by(median_label) %>%
      mutate(reduction = as.numeric(reduction)) %>%
      summarize(average_reduction_median = mean(reduction, na.rm = TRUE))

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_mean, by = "mean_label") %>%
      rename(average_reduction_by_mean = average_reduction_mean)

    # Join this average reduction back to the original data frame
    df <- df %>%
      left_join(average_reduction_median, by = "median_label") %>%
      rename(average_reduction_by_median = average_reduction_median)

    # Joining the errors and latency summaries by Cohort
    df <- df %>%
      left_join(df_errors_summary, by = c("cohort" = "Cohort")) %>%
      left_join(df_latency_summary, by = c("cohort" = "Cohort"))
  }

  # Return the processed dataframe
  return(df)
}

# Apply the function to each dataframe in the list
processed_dataframes <- lapply(all_dfs, function(df) {
  process_dataframe_map(df, errors_summary = df_errors_summary, latency_summary = df_latency_summary, k = 2, print_results = TRUE, plot_silhouette = FALSE)
})


# Remove any dataframes that contain NA values
processed_dataframes <- keep(processed_dataframes, function(df) !any(is.na(df)))
```

# Plot for kmeans_label by average latencty
```{r kmeans label by latency plot}
# Keep dataframes with > 4 columns
processed_dataframes <- keep(processed_dataframes, function(df) {
  ncol(df) > 4
})

all_processed_dfs <- bind_rows(processed_dataframes)
all_processed_dfs_filtered <- filter(all_processed_dfs, average_reduction_by_kmeans < 10)


# Function to plot all the data
plot_correlation <- function(data, x_var, y_var, calculate_correlation = TRUE, correlation_method = "spearman", plot_color = cohort, plot_shape = kmeans_label, point_size = 2, x_label, y_label, plot_title, subregion = NULL) {
  if (calculate_correlation) {
    correlation_result <- cor.test(data[[deparse(substitute(x_var))]], data[[deparse(substitute(y_var))]], method = correlation_method)
  }

  # Formatting the relevant label titles from the data input
  color_label <- str_to_title(deparse(substitute(plot_color)))
  shape_label <- str_to_title(deparse(substitute(plot_shape)))
  color_label <- gsub(x = color_label, pattern = "_", replacement = " ")
  shape_label <- gsub(x = shape_label, pattern = "_", replacement = " ")
  x_label <- str_to_title(deparse(substitute(x_var)))
  x_label <- gsub(x = x_label, pattern = "_", replacement = " ")
  y_label <- str_to_title(deparse(substitute(y_var)))
  y_label <- gsub(x = y_label, pattern = "_", replacement = " ")
  plot_title <- noquote(str_to_title(paste(x_label, "by", y_label)))

  p <- ggplot(data, aes(x = {{ x_var }}, y = {{ y_var }}, color = {{ plot_color }}, shape = {{ plot_shape }}, size = point_size)) +
    geom_point() +
    theme(
      plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 18, hjust = 0.5),
      axis.title = element_text(size = 18, face = "bold"),
      axis.text = element_text(size = 16),
      legend.title = element_text(size = 18, face = "bold"),
      legend.text = element_text(size = 16),
      panel.background = element_blank(),
      panel.grid.major.x = element_line(color = "black"),
      panel.grid.major.y = element_line(color = "black")
    ) +
    labs(
      x = x_label, y = y_label,
      title = plot_title,
      subtitle = ifelse(
        calculate_correlation,
        paste(
          str_to_title(correlation_method), "=", round(correlation_result$estimate, 2),
          "p-value =", signif(correlation_result$p.value, digits = 3)
        ),
        NULL
      )
    ) +
    scale_color_discrete(name = color_label) +
    scale_shape_discrete(name = shape_label) +
    guides(size = "none")

  ggsave(plot = p, filename = paste(tolower(plot_title), "_", correlation_method, "_", subregion,"_", unique(data$cohort)  ,"combined_cohort_map.png"), device = "png", width = 8, height = 8, units = "in", dpi = 600, bg = "white", path = "Outputs/Plots/exploratory_plots/")
}


# Apply the function to each dataframe in the list
plots_list <- lapply(processed_dataframes, function(df) {
  plot_correlation(
    data = df,
    x_var = average_reduction_by_mean,
    y_var = total_errors,
    calculate_correlation = TRUE, # Set to TRUE to calculate correlation
    correlation_method = "spearman", # Set correlation method
    plot_color = mean_label,
    plot_shape = NULL,
    subregion = df$subregion
  )
})

p_combined_1 <-  plot_correlation(
    data = all_processed_dfs_filtered,
    x_var = average_reduction_by_median,
    y_var = avg_latency,
    calculate_correlation = TRUE, # Set to TRUE to calculate correlation
    correlation_method = "spearman", # Set correlation method
    plot_color = median_label,
    plot_shape = NULL,
    subregion = NULL
  )
```
